{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70694b67",
   "metadata": {},
   "source": [
    "Code inspired by https://github.com/UoB-CS-AVAI/Week2-train-Deep-Neural-Network-to-denoise-image and https://github.com/DmitryUlyanov/deep-image-prior\n",
    "\n",
    "The model architectures (skip, unet, resnet) and utils functions are directly copied from these githubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d173f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (12.0.0)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: lpips in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (3.10.7)\n",
      "Collecting models\n",
      "  Using cached models-0.9.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [8 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 35, in <module>\n",
      "        File \"C:\\Users\\blobf.DESKTOP-IUEL8R6\\AppData\\Local\\Temp\\pip-install-8q3wi8ft\\models_0d20757c3ae2494c8a442aa4f67535a7\\setup.py\", line 25, in <module>\n",
      "          import models\n",
      "        File \"C:\\Users\\blobf.DESKTOP-IUEL8R6\\AppData\\Local\\Temp\\pip-install-8q3wi8ft\\models_0d20757c3ae2494c8a442aa4f67535a7\\models\\__init__.py\", line 23, in <module>\n",
      "          from base import *\n",
      "      ModuleNotFoundError: No module named 'base'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision pillow scikit-image lpips matplotlib \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b9abc",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061062df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import lpips\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from utils.sr_utils import * \n",
    "from utils.common_utils import *\n",
    "\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070fbdd",
   "metadata": {},
   "source": [
    "Get LR dataset and HR dataset (for ground truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478cb24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2KDataset(Dataset):\n",
    "    def __init__(self, hr_dir, lr_dir):\n",
    "        self.hr_paths = sorted(glob.glob(os.path.join(hr_dir, '*.png')))\n",
    "        self.lr_paths = sorted(glob.glob(os.path.join(lr_dir, '*.png')))\n",
    "\n",
    "        self.transform = transforms.ToTensor() # PIL to Tensor\n",
    "    \n",
    "    def __len__(self): # dataloader needs access to length\n",
    "        return len(self.hr_paths)\n",
    "\n",
    "    def __getitem__(self, index): # dataloader needs access to dataset items by index\n",
    "        lr_img = self.transform(Image.open(self.lr_paths[index])) # DIV2K is RGB images\n",
    "        hr_img = self.transform(Image.open(self.hr_paths[index]))\n",
    "        return lr_img, hr_img\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "\n",
    "train_dataset = DIV2KDataset(\n",
    "    hr_dir=str(DATA_DIR / 'DIV2K_train_HR'),\n",
    "    lr_dir=str(DATA_DIR / 'DIV2K_train_LR_x8')\n",
    ")\n",
    "\n",
    "val_dataset = DIV2KDataset(\n",
    "    hr_dir=str(DATA_DIR / 'DIV2K_valid_HR'),\n",
    "    lr_dir=str(DATA_DIR / 'DIV2K_valid_LR_x8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4a22c",
   "metadata": {},
   "source": [
    "Get data - 1 randomly selected image (LR as x0 and corresponding HR for PSNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cdfe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random index\n",
    "index = random.randint(0, len(train_dataset) - 1)\n",
    "print(f\"Selecting image index: {index} from dataset\")\n",
    "\n",
    "# get the LR and HR images at that index\n",
    "img_LR_tensor, img_HR_tensor = train_dataset[index]\n",
    "\n",
    "# convert from [C, H,W] to [1, C, H, W] and move to GPU\n",
    "img_LR_var = img_LR_tensor.unsqueeze(0).to(device)\n",
    "img_HR_var = img_HR_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "print(f\"HR Image Shape: {img_HR_var.shape}\")\n",
    "print(f\"LR Input Shape: {img_LR_var.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2dfa95",
   "metadata": {},
   "source": [
    "Define network hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4fd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = 'noise' # choice of 'noise' or 'meshgrid' - just use noise for DIP\n",
    "pad = 'reflection' # choice of padding type, which \n",
    "OPT_OVER = 'net' # 'net' - optimise the network weights. 'net,input' - optimise the noise too. \n",
    "\n",
    "reg_noise_std = 1./30. # std of noise added to input at each iteration\n",
    "LR = 0.01 # learning rate for optimizer\n",
    "OPTIMIZER = 'adam' # 'adam' or 'LBFGS'\n",
    "show_every = 100 # how often to show results\n",
    "num_iter = 2000 # total iterations \n",
    "input_depth = 32 # number of channels in input noise \n",
    "figsize = 4 # figure size for plotting\n",
    "\n",
    "NET_TYPE = 'skip' # choice of 'skip', 'resnet' or 'unet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26ca73",
   "metadata": {},
   "source": [
    "Define input noise, z - dimensions of HR image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_input = get_noise(input_depth, INPUT, (img_HR_var.shape[2], img_HR_var.shape[3])).type(dtype).detach()\n",
    "\n",
    "print(\"Input noise shape:\", net_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a27d1",
   "metadata": {},
   "source": [
    "Define network architecture (skip, resnet or unet) and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(input_depth, NET_TYPE, pad, skip_n33d=128, skip_n33u=128, skip_n11=4, num_scales=5, upsample_mode='bilinear'):\n",
    "    if NET_TYPE == 'skip':\n",
    "        return skip(input_depth, 3, \n",
    "               num_channels_down = [skip_n33d] * num_scales, \n",
    "               num_channels_up =   [skip_n33u] * num_scales,\n",
    "               num_channels_skip =    [skip_n11] * num_scales, \n",
    "               filter_size_up = 3, filter_size_down = 3, \n",
    "               upsample_mode=upsample_mode, filter_skip_size=1,\n",
    "               need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU')\n",
    "    elif NET_TYPE == 'resnet':\n",
    "        return resnet(input_depth, 3, num_channels=128, num_blocks=8, act_fun='LeakyReLU')\n",
    "    elif NET_TYPE == 'unet':\n",
    "        return unet(input_depth, 3, num_channels=[128]*5, act_fun='LeakyReLU')\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "# initialise network\n",
    "net = get_net(input_depth, NET_TYPE, pad,\n",
    "              skip_n33d=128,\n",
    "              skip_n33u=128,\n",
    "              skip_n11=4,\n",
    "              num_scales=5,\n",
    "              upsample_mode='bilinear').type(dtype)\n",
    "\n",
    "# print number of parameters\n",
    "s  = sum([np.prod(list(p.size())) for p in net.parameters()]); \n",
    "print ('Number of params: %d' % s)\n",
    "\n",
    "# loss function\n",
    "mse = torch.nn.MSELoss().type(dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482f705",
   "metadata": {},
   "source": [
    "Define degradation function, H - known to be bicubic x8 downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degradation_operator(hr_tensor): \n",
    "    return torch.nn.functional.interpolate(\n",
    "        hr_tensor, # the tensor of the HR x_hat output by our model\n",
    "        scale_factor=1/8, \n",
    "        mode='bicubic', \n",
    "        align_corners=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce6f2e9",
   "metadata": {},
   "source": [
    "Get loss: MSE between x0 (original LR image) and model output x_hat downsampled by H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20cb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss().type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19564126",
   "metadata": {},
   "source": [
    "Training loop - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c397e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LPIPS metric (make sure to import lpips first)\n",
    "loss_fn_lpips = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "# Initialize history lists\n",
    "psnr_history = [] \n",
    "ssim_history = []\n",
    "lpips_history = []\n",
    "\n",
    "net_input_saved = net_input.detach().clone()\n",
    "noise = net_input.detach().clone()\n",
    "\n",
    "i = 0\n",
    "\n",
    "# define closure function (runs once per iteration inside \"optimize\")\n",
    "def closure():\n",
    "    global i, net_input, net_input_saved, noise\n",
    "    \n",
    "    # add noise to input for regularisation\n",
    "    if reg_noise_std > 0:\n",
    "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "    \n",
    "    # forward pass to generate HR estimate\n",
    "    out_HR = net(net_input)\n",
    "    \n",
    "    # downsample the hr output to make it the same dimensions as our lr input\n",
    "    out_LR = degradation_operator(out_HR)\n",
    "    \n",
    "    # compare downsampled model output vs original lr input\n",
    "    total_loss = mse(out_LR, img_LR_var)\n",
    "    \n",
    "    # backpropagate loss\n",
    "    total_loss.backward()\n",
    "\n",
    "    if i % show_every == 0:\n",
    "        out_HR_np = torch_to_np(out_HR)\n",
    "        img_HR_np = torch_to_np(img_HR_var)\n",
    "\n",
    "        # calculate PSNR\n",
    "        mse_val = np.mean((img_HR_np - out_HR_np) ** 2)\n",
    "        psnr_HR = 10 * np.log10(1 / mse_val)\n",
    "        psnr_history.append(psnr_HR)\n",
    "\n",
    "        # calculate SSIM\n",
    "        # Transpose (C, H, W) -> (H, W, C) for SSIM\n",
    "        current_ssim = ssim(\n",
    "            img_HR_np.transpose(1, 2, 0), \n",
    "            out_HR_np.transpose(1, 2, 0), \n",
    "            data_range=1.0, \n",
    "            channel_axis=2\n",
    "        )\n",
    "        ssim_history.append(current_ssim)\n",
    "\n",
    "        # calculate LPIPS\n",
    "        # normalise from [0, 1] to [-1, 1] for LPIPS\n",
    "        with torch.no_grad():\n",
    "            current_lpips = loss_fn_lpips(\n",
    "                out_HR * 2 - 1, \n",
    "                img_HR_var * 2 - 1\n",
    "            ).item()\n",
    "        lpips_history.append(current_lpips)\n",
    "        \n",
    "        print(f'Iter {i:05d} | Loss {total_loss.item():.6f} | PSNR: {psnr_HR:.2f} | SSIM: {current_ssim:.4f} | LPIPS: {current_lpips:.4f}')\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "p = get_params(OPT_OVER, net, net_input)\n",
    "optimize(OPTIMIZER, p, closure, LR, num_iter)\n",
    "\n",
    "# get final result\n",
    "out_HR_final = np.clip(torch_to_np(net(net_input)), 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11920e90",
   "metadata": {},
   "source": [
    "Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eace7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "#display hr\n",
    "ax[0].imshow(torch_to_np(img_HR_var).transpose(1, 2, 0))\n",
    "ax[0].set_title(f\"Ground Truth HR\")\n",
    "ax[0].axis('off')\n",
    "\n",
    "# lr\n",
    "display_LR = torch.nn.functional.interpolate(img_LR_var, size=img_HR_var.shape[2:], mode='nearest')\n",
    "ax[1].imshow(torch_to_np(display_LR).transpose(1, 2, 0))\n",
    "ax[1].set_title(f\"Input LR (x8 bicubic down)\")\n",
    "ax[1].axis('off')\n",
    "\n",
    "# DIP output\n",
    "ax[2].imshow(out_HR_final.transpose(1, 2, 0))\n",
    "ax[2].set_title(f\"DIP Output HR\\nPSNR: {psnr_history[-1]:.2f} dB\")\n",
    "ax[2].axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot PSNR\n",
    "ax[0].plot(psnr_history, color='g')\n",
    "ax[0].set_title(\"PSNR (Higher is better)\")\n",
    "ax[0].set_xlabel(\"Iteration (x100)\")\n",
    "ax[0].set_ylabel(\"dB\")\n",
    "ax[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot SSIM\n",
    "ax[1].plot(ssim_history, color='b')\n",
    "ax[1].set_title(\"SSIM (Higher is better)\")\n",
    "ax[1].set_xlabel(\"Iteration (x100)\")\n",
    "ax[1].set_ylabel(\"Index\")\n",
    "ax[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot LPIPS\n",
    "ax[2].plot(lpips_history, color='r')\n",
    "ax[2].set_title(\"LPIPS (Lower is better)\")\n",
    "ax[2].set_xlabel(\"Iteration (x100)\")\n",
    "ax[2].set_ylabel(\"Distance\")\n",
    "ax[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avai-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
