{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a7cc15",
   "metadata": {},
   "source": [
    "Code inspired by https://github.com/assafshocher/ZSSR and https://github.com/jacobgil/pytorch-zssr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d173f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (12.0.0)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: lpips in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (3.10.7)\n",
      "Collecting models\n",
      "  Using cached models-0.9.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [8 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 35, in <module>\n",
      "        File \"C:\\Users\\blobf.DESKTOP-IUEL8R6\\AppData\\Local\\Temp\\pip-install-8q3wi8ft\\models_0d20757c3ae2494c8a442aa4f67535a7\\setup.py\", line 25, in <module>\n",
      "          import models\n",
      "        File \"C:\\Users\\blobf.DESKTOP-IUEL8R6\\AppData\\Local\\Temp\\pip-install-8q3wi8ft\\models_0d20757c3ae2494c8a442aa4f67535a7\\models\\__init__.py\", line 23, in <module>\n",
      "          from base import *\n",
      "      ModuleNotFoundError: No module named 'base'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision pillow scikit-image lpips matplotlib \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b9abc",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061062df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from utils.sr_utils import * \n",
    "from utils.common_utils import *\n",
    "\n",
    "# Standard GPU check (using Lab syntax 'dtype')\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070fbdd",
   "metadata": {},
   "source": [
    "Get LR dataset and HR dataset (for ground truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478cb24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2KDataset(Dataset):\n",
    "    def __init__(self, hr_dir, lr_dir):\n",
    "        self.hr_paths = sorted(glob.glob(os.path.join(hr_dir, '*.png')))\n",
    "        self.lr_paths = sorted(glob.glob(os.path.join(lr_dir, '*.png')))\n",
    "\n",
    "        self.transform = transforms.ToTensor() # PIL to Tensor\n",
    "    \n",
    "    def __len__(self): # dataloader needs access to length\n",
    "        return len(self.hr_paths)\n",
    "\n",
    "    def __getitem__(self, index): # dataloader needs access to dataset items by index\n",
    "        lr_img = self.transform(Image.open(self.lr_paths[index])) # DIV2K is RGB images\n",
    "        hr_img = self.transform(Image.open(self.hr_paths[index]))\n",
    "        return lr_img, hr_img\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "\n",
    "train_dataset = DIV2KDataset(\n",
    "    hr_dir=str(DATA_DIR / 'DIV2K_train_HR'),\n",
    "    lr_dir=str(DATA_DIR / 'DIV2K_train_LR_x8')\n",
    ")\n",
    "\n",
    "val_dataset = DIV2KDataset(\n",
    "    hr_dir=str(DATA_DIR / 'DIV2K_valid_HR'),\n",
    "    lr_dir=str(DATA_DIR / 'DIV2K_valid_LR_x8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4a22c",
   "metadata": {},
   "source": [
    "Get data - 1 randomly selected image (LR as x0 and corresponding HR for PSNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cdfe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random index\n",
    "index = random.randint(0, len(train_dataset) - 1)\n",
    "print(f\"Selecting image index: {index} from dataset\")\n",
    "\n",
    "# get the LR and HR images at that index\n",
    "img_LR_tensor, img_HR_tensor = train_dataset[index]\n",
    "\n",
    "# convert from [C, H,W] to [1, C, H, W] and move to GPU\n",
    "img_LR_var = img_LR_tensor.unsqueeze(0).to(device)\n",
    "img_HR_var = img_HR_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "print(f\"HR Image Shape: {img_HR_var.shape}\")\n",
    "print(f\"LR Input Shape: {img_LR_var.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2dfa95",
   "metadata": {},
   "source": [
    "Define network hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f657939",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_FACTOR = 4        # We want to go from LR -> HR (x4)\n",
    "EPOCHS = 15\n",
    "CROPS_PER_EPOCH = 500   # Number of training examples extracted per epoch\n",
    "LEARNING_RATE = 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c864a",
   "metadata": {},
   "source": [
    "Define degradation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab11d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degradation(img_tensor, scale=0.5):\n",
    "    # downsamples an image by \"scale\" to create a lower-resolution version\n",
    "    # so the network can learn how to reverse the degradation\n",
    "    return TF.interpolate(\n",
    "        img_tensor,\n",
    "        scale_factor=scale,\n",
    "        mode='bicubic',\n",
    "        align_corners=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292da8b",
   "metadata": {},
   "source": [
    "Create the internal dataset for the image - smaller crops of the lr image are downsampled to learn the map of super lr to lr, which will then later be applied to lr to hr. \n",
    "\n",
    "For this stage, we treat the lr image as the ground truth and try to learn the mapping to it from smaller even lower resolution images.\n",
    "\n",
    "We use data augmentation here (flips and rotations) in order to get more training data to learn our mapping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afa95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZSSRInternalDataset(Dataset):\n",
    "    def __init__(self, target_img, num_samples=1000, crop_size=64):\n",
    "        self.target = target_img\n",
    "        self.num_samples = num_samples\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # randomly crop the target (our original lr image)\n",
    "        _, _, h, w = self.target.shape\n",
    "        \n",
    "        cs = self.crop_size\n",
    "        top = random.randint(0, max(0, h - cs - 1))\n",
    "        left = random.randint(0, max(0, w - cs - 1))\n",
    "\n",
    "        hr_crop = self.target[:, :, top:top+cs, left:left+cs]\n",
    "\n",
    "        # create the input by degrading/downsampling the cropped area\n",
    "        lr_crop = degradation(hr_crop, scale=0.5)\n",
    "\n",
    "        # squeeze to (C, H, W) for augmentation\n",
    "        hr_crop = hr_crop.squeeze(0)\n",
    "        lr_crop = lr_crop.squeeze(0)\n",
    "\n",
    "        # data augmentation\n",
    "        if random.random() > 0.5: # Horizontal Flip\n",
    "            hr_crop = TF.hflip(hr_crop)\n",
    "            lr_crop = TF.hflip(lr_crop)\n",
    "        if random.random() > 0.5: # Vertical Flip\n",
    "            hr_crop = TF.vflip(hr_crop)\n",
    "            lr_crop = TF.vflip(lr_crop)\n",
    "        if random.random() > 0.5: # 90-degree Rotation\n",
    "            hr_crop = torch.rot90(hr_crop, 1, [1, 2])\n",
    "            lr_crop = torch.rot90(lr_crop, 1, [1, 2])\n",
    "\n",
    "        return lr_crop, hr_crop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21612e98",
   "metadata": {},
   "source": [
    "Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17243e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZSSRNet(nn.Module):\n",
    "    def __init__(self, channels=64):\n",
    "        super(ZSSRNet, self).__init__()\n",
    "        \n",
    "        # head\n",
    "        self.head = nn.Conv2d(3, channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        # body \"We use a simple, fully convolutional network, with 8 hidden layers\"\n",
    "        body_layers = []\n",
    "        for _ in range(8):\n",
    "            body_layers.append(nn.Conv2d(channels, channels, kernel_size=3, padding=1))\n",
    "            body_layers.append(nn.ReLU(inplace=True))\n",
    "        self.body = nn.Sequential(*body_layers)\n",
    "        \n",
    "        # tail (predicts residual, i.e. corrections)\n",
    "        self.tail = nn.Conv2d(channels, 3, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # bicubic upsample - \"base guess\" is direct upsampling - this will be blurry but we can learn improvements\n",
    "        x_upscaled = TF.interpolate(x, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "        \n",
    "        # predict residual (corrections to the blurry upsampled image)\n",
    "        feat = self.head(x_upscaled)\n",
    "        feat = self.body(feat)\n",
    "        residual = self.tail(feat)\n",
    "        \n",
    "        # add residual to base \n",
    "        return x_upscaled + residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f73f5",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use lr image as the training target\n",
    "model_zssr = ZSSRNet().to(device)\n",
    "optimizer_zssr = torch.optim.Adam(model_zssr.parameters(), lr=LEARNING_RATE)\n",
    "zssr_ds = ZSSRInternalDataset(img_LR_var, num_samples=CROPS_PER_EPOCH)\n",
    "zssr_loader = DataLoader(zssr_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "loss_history = []\n",
    "print(\"Starting ZSSR Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    model_zssr.train()\n",
    "    \n",
    "    for i, (lr_batch, hr_batch) in enumerate(zssr_loader):\n",
    "        lr_batch, hr_batch = lr_batch.to(device), hr_batch.to(device)\n",
    "        \n",
    "        output = model_zssr(lr_batch)\n",
    "        \n",
    "        # L1 loss like in the original paper\n",
    "        loss = TF.l1_loss(output, hr_batch)\n",
    "        \n",
    "        optimizer_zssr.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_zssr.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(zssr_loader)\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "    if epoch % 3 == 0 or epoch == EPOCHS - 1:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Run a quick test on the full LR image (x2 scale)\n",
    "        model_zssr.eval()\n",
    "        with torch.no_grad():\n",
    "            test_out = model_zssr(img_LR_var)\n",
    "            disp_pred = np.clip(test_out.squeeze(0).permute(1, 2, 0).cpu().numpy(), 0, 1)\n",
    "            \n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(loss_history, label='L1 Loss')\n",
    "            plt.title(f\"Training Loss (Epoch {epoch})\")\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(disp_pred)\n",
    "            plt.title(f\"Internal Validation (x2) | Epoch {epoch}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e143c6",
   "metadata": {},
   "source": [
    "Inference - 3 passes: x2, x4, x8 to reach from LR to HR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell: Final Inference (x8 Upscaling) ---\n",
    "\n",
    "print(\"Running ZSSR Inference (x8 Gradual)...\")\n",
    "model_zssr.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Pass 1: LR -> x2\n",
    "    print(\"Upscaling Step 1/3 (x2)...\")\n",
    "    sr_x2 = model_zssr(img_LR_var)\n",
    "    \n",
    "    # Pass 2: x2 -> x4\n",
    "    # feed the result of step 1 back into the network\n",
    "    print(\"Upscaling Step 2/3 (x4)...\")\n",
    "    sr_x4 = model_zssr(sr_x2)\n",
    "    \n",
    "    # Pass 3: x4 -> x8 \n",
    "    # feed the result of step 2 back into the network\n",
    "    print(\"Upscaling Step 3/3 (x8)...\")\n",
    "    sr_x8 = model_zssr(sr_x4)\n",
    "    \n",
    "    # resize output to match HR ground truth dimensions for calculaltions\n",
    "    # LR is 175x175,  175 * 8 = 1400, but HR is 1404 x 1404\n",
    "    target_h, target_w = img_HR_var.shape[2], img_HR_var.shape[3]\n",
    "    \n",
    "    final_sr = TF.interpolate(\n",
    "        sr_x8, \n",
    "        size=(target_h, target_w), \n",
    "        mode='bicubic', \n",
    "        align_corners=False\n",
    "    )\n",
    "    final_sr = torch.clamp(final_sr, 0, 1)\n",
    "\n",
    "# convert to numpy\n",
    "sr_np = final_sr.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "gt_np = img_HR_var.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "lr_np = img_LR_var.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "mse = np.mean((gt_np - sr_np) ** 2)\n",
    "if mse == 0:\n",
    "    final_psnr = 100\n",
    "else:\n",
    "    final_psnr = 20 * np.log10(1.0 / np.sqrt(mse))\n",
    "\n",
    "print(f\"Final ZSSR PSNR (x8): {final_psnr:.2f} dB\")\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(lr_np)\n",
    "plt.title(f\"LR Input (x8 smaller)\\n{lr_np.shape}\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(sr_np)\n",
    "plt.title(f\"ZSSR Output (x8)\\nPSNR: {final_psnr:.2f} dB\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(gt_np)\n",
    "plt.title(f\"Ground Truth\\n{gt_np.shape}\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avai-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
