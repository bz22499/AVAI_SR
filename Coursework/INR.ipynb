{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d173f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (12.0.0)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: lpips in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\blobf.desktop-iuel8r6\\anaconda3\\envs\\avai-lab\\lib\\site-packages (3.10.7)\n",
      "Collecting models\n",
      "  Using cached models-0.9.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [8 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 35, in <module>\n",
      "        File \"C:\\Users\\blobf.DESKTOP-IUEL8R6\\AppData\\Local\\Temp\\pip-install-8q3wi8ft\\models_0d20757c3ae2494c8a442aa4f67535a7\\setup.py\", line 25, in <module>\n",
      "          import models\n",
      "        File \"C:\\Users\\blobf.DESKTOP-IUEL8R6\\AppData\\Local\\Temp\\pip-install-8q3wi8ft\\models_0d20757c3ae2494c8a442aa4f67535a7\\models\\__init__.py\", line 23, in <module>\n",
      "          from base import *\n",
      "      ModuleNotFoundError: No module named 'base'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision pillow scikit-image lpips matplotlib \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b9abc",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061062df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.ndimage import laplace, sobel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image, ImageFilter\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070fbdd",
   "metadata": {},
   "source": [
    "Get LR dataset and HR dataset (for ground truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478cb24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2KDataset(Dataset):\n",
    "    def __init__(self, hr_dir, lr_dir):\n",
    "        self.hr_paths = sorted(glob.glob(os.path.join(hr_dir, '*.png')))\n",
    "        self.lr_paths = sorted(glob.glob(os.path.join(lr_dir, '*.png')))\n",
    "\n",
    "        self.transform = transforms.ToTensor() # PIL to Tensor\n",
    "    \n",
    "    def __len__(self): # dataloader needs access to length\n",
    "        return len(self.hr_paths)\n",
    "\n",
    "    def __getitem__(self, index): # dataloader needs access to dataset items by index\n",
    "        lr_img = self.transform(Image.open(self.lr_paths[index])) # DIV2K is RGB images\n",
    "        hr_img = self.transform(Image.open(self.hr_paths[index]))\n",
    "        return lr_img, hr_img\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "\n",
    "train_dataset = DIV2KDataset(\n",
    "    hr_dir=str(DATA_DIR / 'DIV2K_train_HR'),\n",
    "    lr_dir=str(DATA_DIR / 'DIV2K_train_LR_x8')\n",
    ")\n",
    "\n",
    "val_dataset = DIV2KDataset(\n",
    "    hr_dir=str(DATA_DIR / 'DIV2K_valid_HR'),\n",
    "    lr_dir=str(DATA_DIR / 'DIV2K_valid_LR_x8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf7bc5",
   "metadata": {},
   "source": [
    "Get data - 1 randomly selected image (LR as x0 and corresponding HR for PSNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc9fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random index\n",
    "index = random.randint(0, len(train_dataset) - 1)\n",
    "print(f\"Selecting image index: {index} from dataset\")\n",
    "\n",
    "# get the LR and HR images at that index\n",
    "img_LR_tensor, img_HR_tensor = train_dataset[index]\n",
    "\n",
    "# convert from [C, H,W] to [1, C, H, W] and move to GPU\n",
    "img_LR_var = img_LR_tensor.unsqueeze(0).to(device)\n",
    "img_HR_var = img_HR_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "print(f\"HR Image Shape: {img_HR_var.shape}\")\n",
    "print(f\"LR Input Shape: {img_LR_var.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5082ae5",
   "metadata": {},
   "source": [
    "SIREN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d251326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineLayer(nn.Module):\n",
    "    \"\"\" Linear layer followed by the sine activation\n",
    "\n",
    "    If `is_first == True`, then it represents the first layer of the network.\n",
    "    In this case, omega_0 is a frequency factor, which simply multiplies the activations before the nonlinearity.\n",
    "    Note that it influences the initialization scheme.\n",
    "\n",
    "    If `is_first == False`, then the weights will be divided by omega_0 so as to keep the magnitude of activations constant,\n",
    "    but boost gradients to the weight matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "        self.is_first = is_first\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        self.init_weights()\n",
    "\n",
    "    # initialize weights uniformly\n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                self.linear.weight.uniform_(-1 / self.in_features,\n",
    "                                             1 / self.in_features)\n",
    "            else:\n",
    "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,\n",
    "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # 1. pass input through linear layer (self.linear layer performs the linear transformation on the input)\n",
    "        x = self.linear(input)\n",
    "\n",
    "        # 2. scale the output of the linear transformation by the frequency factor\n",
    "        x = x * self.omega_0\n",
    "\n",
    "        # 3. apply sine activation\n",
    "        x = torch.sin(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f0524",
   "metadata": {},
   "source": [
    "SIREN network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afae06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siren(nn.Module):\n",
    "    \"\"\" SIREN architecture \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, hidden_features=256, hidden_layers=3, outermost_linear=False,\n",
    "                 first_omega_0=30, hidden_omega_0=30.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = []\n",
    "        self.net.append(SineLayer(in_features, hidden_features,\n",
    "                                  is_first=True, omega_0=first_omega_0))\n",
    "\n",
    "        for i in range(hidden_layers):\n",
    "            self.net.append(SineLayer(hidden_features, hidden_features,\n",
    "                                      is_first=False, omega_0=hidden_omega_0))\n",
    "\n",
    "        if outermost_linear:\n",
    "            final_linear = nn.Linear(hidden_features, out_features)\n",
    "            with torch.no_grad():\n",
    "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0,\n",
    "                                              np.sqrt(6 / hidden_features) / hidden_omega_0)\n",
    "            self.net.append(final_linear)\n",
    "        else:\n",
    "            self.net.append(SineLayer(hidden_features, out_features,\n",
    "                                      is_first=False, omega_0=hidden_omega_0))\n",
    "\n",
    "        self.net = nn.Sequential(*self.net) # sequential wrapper of SineLayer and Linear\n",
    "\n",
    "    def forward(self, coords):\n",
    "        coords = coords.clone().detach().requires_grad_(True)\n",
    "        output = self.net(coords)\n",
    "        return output, coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4637a2",
   "metadata": {},
   "source": [
    "Define helper functions to create 2d coordinate grid and normalise images to [-1, 1] (i.e. mean and std. of 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mgrid(sidelen1,sidelen2, dim=2):\n",
    "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.\n",
    "    sidelen: int\n",
    "    dim: int'''\n",
    "\n",
    "    if sidelen1 >= sidelen2:\n",
    "      # use sidelen1 steps to generate the grid\n",
    "      tensors = tuple(dim * [torch.linspace(-1, 1, steps = sidelen1)])\n",
    "      mgrid = torch.stack(torch.meshgrid(*tensors), dim = -1)\n",
    "      # crop it along one axis to fit sidelen2\n",
    "      minor = int((sidelen1 - sidelen2)/2)\n",
    "      mgrid = mgrid[:,minor:sidelen2 + minor]\n",
    "\n",
    "    if sidelen1 < sidelen2:\n",
    "      tensors = tuple(dim * [torch.linspace(-1, 1, steps = sidelen2)])\n",
    "      mgrid = torch.stack(torch.meshgrid(*tensors), dim = -1)\n",
    "\n",
    "      minor = int((sidelen2 - sidelen1)/2)\n",
    "      mgrid = mgrid[minor:sidelen1 + minor,:]\n",
    "\n",
    "    # flatten the gird\n",
    "    mgrid = mgrid.reshape(-1, dim)\n",
    "\n",
    "    return mgrid\n",
    "\n",
    "def image_to_tensor(img):\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(torch.Tensor([0.5]), torch.Tensor([0.5]))\n",
    "    ])\n",
    "    img = transform(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690efc5",
   "metadata": {},
   "source": [
    "Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4318c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1 get dimensions\n",
    "_, c, lr_h, lr_w = img_LR_var.shape # lr image is [1, 3, H, W]\n",
    "_, _, hr_h, hr_w = img_HR_var.shape\n",
    "\n",
    "# 2 create coordinate grids (to use as the inputs)\n",
    "lr_coords = get_mgrid(lr_h, lr_w).to(device) # Inputs for training\n",
    "hr_coords = get_mgrid(hr_h, hr_w).to(device) # Inputs for super-resolution (inference)\n",
    "\n",
    "# 3 format pixel values (the targets for the network)\n",
    "\n",
    "# flatten image pixels to match list of coordinates.\n",
    "# permute to (H, W, C), then flatten to (N, 3) - i.e. a list of RGB values for each pixel coordinate\n",
    "lr_pixels = img_LR_var.squeeze(0).permute(1, 2, 0).reshape(-1, 3)\n",
    "hr_pixels = img_HR_var.squeeze(0).permute(1, 2, 0).reshape(-1, 3)\n",
    "\n",
    "# normalise from [0, 1] to [-1, 1]\n",
    "lr_pixels = (lr_pixels - 0.5) / 0.5\n",
    "hr_pixels = (hr_pixels - 0.5) / 0.5\n",
    "\n",
    "print(f\"Training Input (Coords): {lr_coords.shape}\") # Should be (N_lr, 2)\n",
    "print(f\"Training Target (Pixels): {lr_pixels.shape}\") # Should be (N_lr, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a03ea4b",
   "metadata": {},
   "source": [
    "Initialise and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise model\n",
    "model = Siren(in_features=2, out_features=3, hidden_features=256, \n",
    "              hidden_layers=3, outermost_linear=True).to(device)\n",
    "\n",
    "# adam as optimiser\n",
    "optim_inr = torch.optim.Adam(lr=1e-4, params=model.parameters())\n",
    "\n",
    "# training loop\n",
    "steps = 2000 # can be increased\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for step in range(steps):\n",
    "    # 1 input LR coordinates to model\n",
    "    model_output, _ = model(lr_coords)\n",
    "    \n",
    "    # 2 compare output to LR Pixels\n",
    "    loss = ((model_output - lr_pixels)**2).mean() # MSE Loss\n",
    "\n",
    "    # 3 optimise\n",
    "    optim_inr.zero_grad()\n",
    "    loss.backward()\n",
    "    optim_inr.step()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "    # --- VISUALIZATION BLOCK --- #####################################\n",
    "    if step % print_every == 0:\n",
    "        # Switch to evaluation mode (optional but good practice)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # A. Ask model to predict High Res image\n",
    "            sr_output, _ = model(hr_coords)\n",
    "            \n",
    "            # B. Reshape result: (N, 3) -> (H, W, 3)\n",
    "            # We use hr_h and hr_w variables we defined earlier\n",
    "            pred_img = sr_output.view(hr_h, hr_w, 3).cpu().numpy()\n",
    "            \n",
    "            # C. Un-normalize: [-1, 1] -> [0, 1]\n",
    "            pred_img = (pred_img + 1) / 2\n",
    "            pred_img = np.clip(pred_img, 0, 1)\n",
    "\n",
    "            # D. Plotting\n",
    "            clear_output(wait=True) # Clears the previous image\n",
    "            \n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # Plot 1: The current Super-Resolution output\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title(f\"Step {step} | Loss: {loss.item():.5f}\")\n",
    "            plt.imshow(pred_img)\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Plot 2: The Ground Truth (for comparison)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title(\"Ground Truth (High Res)\")\n",
    "            # Convert the original HR tensor to numpy for display\n",
    "            gt_display = img_HR_var.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "            plt.imshow(gt_display)\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.show()\n",
    "        \n",
    "        # Switch back to training mode\n",
    "        model.train()\n",
    "\n",
    "        #####################################################################\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3c113",
   "metadata": {},
   "source": [
    "Inference and PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Upscaling...\")\n",
    "\n",
    "# query with HR coordinates\n",
    "with torch.no_grad():\n",
    "    sr_output, _ = model(hr_coords)\n",
    "\n",
    "# reshape back to image format\n",
    "sr_img = sr_output.view(hr_h, hr_w, 3).cpu().numpy()\n",
    "\n",
    "# un-normalise: [-1, 1] back to [0, 1]\n",
    "sr_img = (sr_img + 1) / 2\n",
    "sr_img = np.clip(sr_img, 0, 1)\n",
    "\n",
    "# get ground truth\n",
    "gt_img = img_HR_var.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# PSNR\n",
    "mse = np.mean((gt_img - sr_img) ** 2)\n",
    "sr_psnr = 20 * np.log10(1.0 / np.sqrt(mse))\n",
    "print(f\"Super-Resolution PSNR: {sr_psnr:.2f} dB\")\n",
    "\n",
    "# visualise\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1); plt.title(\"LR Input\"); plt.imshow(img_LR_var.squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
    "plt.subplot(1, 3, 2); plt.title(f\"INR Output ({sr_psnr:.2f} dB)\"); plt.imshow(sr_img)\n",
    "plt.subplot(1, 3, 3); plt.title(\"Ground Truth\"); plt.imshow(gt_img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avai-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
